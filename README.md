# Auto-Scaling Continuous Memory For GUI Agent

<div align="center">

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![arXiv](https://img.shields.io/badge/arXiv-2510.09038-b31b1b.svg)]([https://arxiv.org](https://arxiv.org/abs/2510.09038))
[![arXiv](https://img.shields.io/badge/Website-CoMEMAgent-c8b6ff.svg)](https://wenyiwu0111.github.io/CoMEM-Agent-project-page/)
[![Dataset](https://img.shields.io/badge/ü§ó%20Dataset-GUI--Agent--Trajectories-yellow)](https://huggingface.co/datasets/WenyiWU0111/CoMEM-agent-memory-trajectories)
</div>

<p align="center">
  <img src="CoMEM-Agent-Inference/media/agent_comem_combined.drawio (1).png" alt="GUI-Agent Overview" width="100%">
</p>

This is the official code repository for the paper: [Auto-Scaling Continuous Memory For GUI Agent]().
## üìñ Introduction

We study how to endow GUI agents with **scalable continuous memory** that helps generalize across unfamiliar interfaces. Prior GUI agents compress past trajectories into text tokens, which balloons context length and misses decisive visual cues (*e.g.*, exact widget size and position). 

We propose a **continuous memory** that encodes each GUI trajectory into a fixed-length sequence of continuous embeddings using the VLM itself as an encoder; these embeddings are plugged directly into the backbone's input layer, sharply reducing context cost while preserving fine-grained visual information. As memory size and retrieval depth increase, performance improves monotonically, unlike text memories that degrade with long prompts.

### Key Features

- üéØ **Fixed-length Continuous Memory**: Encode GUI trajectories into compact embeddings (8 continuous tokens)
- üöÄ **Efficient Fine-tuning**: Train only 1.2% of model parameters using LoRA on Q-Former
- üìà **Scalable Performance**: Monotonic improvement with more memory, unlike text-based approaches
- üîÑ **Auto-scaling Data Flywheel**: Discover environments ‚Üí Synthesize tasks ‚Üí Roll out trajectories ‚Üí Verify success
- üí∞ **Cost-effective**: Collect 100K+ trajectories for ~$4000
- üèÜ **SOTA Performance**: Qwen-2.5-VL-7B + continuous memory matches GPT-4o and Claude-4

### Data Flywheel

Our auto-scaling pipeline automatically grows the memory corpus:
1. **Discover**: Find new environments via search
2. **Synthesize**: Generate tasks with open-source VLMs
3. **Roll out**: Execute trajectories with the agent
4. **Verify**: Validate success with the same VLM

## üöÄ Quick Start

### Installation

```bash
# Create environment
conda create -n gui-agent python=3.10
conda activate gui-agent

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers
playwright install
```

## üìä Benchmarks

We evaluate on multiple real-world GUI benchmarks:

### MMInA

- **Shopping** (200 tasks): E-commerce interactions
- **Wikipedia** (308 tasks): Information seeking

### Mind2Web

Cross-website task execution with:
- `test_website`: General websites
- `test_domain_Info`: Information domains
- `test_domain_Service`: Service domains

### WebVoyager

Multi-domain web navigation across 15+ domains including:
- E-commerce (Amazon, Apple)
- Information (ArXiv, Wikipedia, BBC News)
- Services (Booking, GitHub, Google Maps)
- And more...

## üéÆ Running Experiments

### Command Line Interface

Use the main script with flexible options:

```bash
CoMEM-Agent-Inference/run_baseline.sh \
    --eval_type mmina \
    --domain shopping \
    --model qwen2.5-vl \
    --max_steps 15 \
    --use_memory 
```

### Available Options

```bash
Options:
  --eval_type TYPE          Benchmark for Evaluation (mmina, mind2web, webvoyager)
  --domain DOMAIN           Domain for evaluation
  --model MODEL             Model to use
  --max_steps N             Maximum steps per task (default: 15)
  --result_dir DIR          Results directory (default: results)
  --use_memory              Enable memory
  --use_continuous_memory   Enable continuous memory
  --checkpoint_path         Used only when use_continuous_memory is True
  --collect_training_data   Collect trajectory data for memory
  --help, -h                Show help message
```

### Example Scripts

We provide ready-to-use example scripts in the `CoMEM-Agent-Inference/examples/` directory:

#### Baseline Evaluation

```bash
# MMInA Shopping with Qwen2.5-VL
CoMEM-Agent-Inference/examples/run_mmina_shopping.sh

# MMInA Wikipedia
CoMEM-Agent-Inference/examples/run_mmina_wikipedia.sh

# Mind2Web Evaluation
CoMEM-Agent-Inference/examples/run_mind2web.sh

# WebVoyager Evaluation
CoMEM-Agent-Inference/examples/run_webvoyager.sh
```

#### With Memory

```bash
# Text-based memory
CoMEM-Agent-Inference/examples/run_with_text_memory.sh

# Continuous memory (CoMEM)
CoMEM-Agent-Inference/examples/run_with_continuous_memory.sh
```

See [`CoMEM-Agent-Inference/examples/README.md`](CoMEM-Agent-Inference/examples/README.md) for detailed documentation and more examples.

## üì¶ Dataset

We release our auto-collected trajectory dataset on HuggingFace:

**[GUI-Agent-Trajectories](https://huggingface.co/datasets/WenyiWU0111/GUI-Agent-Trajectories)**

This dataset contains **xxx+** GUI interaction trajectories collected through our auto-scaling data flywheel across diverse websites and tasks:

- **Multi-domain Coverage**: E-commerce, information seeking, booking, social media, and more
- **Rich Annotations**: Task descriptions, Website url, Screenshots, model responses, and actions at each step
- **Cost-effective**: Collected 100k+ trajectories for approximately **$4000** using our automated pipeline
- **Self-synthesized**: Tasks generated by open-source VLMs, verified automatically

## ü§ó Pre-trained Checkpoints

We release our continuous memory checkpoints on HuggingFace:

| Model | Base VLM | HuggingFace Link |
|-------|----------|------------------|
| **Qwen2.5-VL + CoMEM** | Qwen2.5-VL-7B-Instruct |  [WenyiWU0111/lora_qformer_test_V4-700_merged](https://huggingface.co/WenyiWU0111/lora_qformer_test_V4-700_merged) |
| **UI-TARS + CoMEM** | UI-TARS-V1.5-7B |  [WenyiWU0111/lora_qformer_uitars_test_V1-400_merged](https://huggingface.co/WenyiWU0111/lora_qformer_uitars_test_V1-400_merged) |


## üîß Supported Models

### Open-Source VLMs

- **Qwen2.5-VL-7B** / **Qwen2.5-VL-32B**: SOTA vision-language models
- **Qwen2-VL-7B**: Previous generation Qwen VL
- **UI-TARS-V1.5-7B**: Specialized for GUI understanding
- **CogAgent-9B**: Multi-modal agent model
- **WebSight-7B**: Web-specific VLM

### Commercial APIs

- **GPT-4o**: OpenAI's multimodal model
- **Claude-3.5-Sonnet / Claude-4**: Anthropic's model
- **Gemini-2.5-Pro**: Google's latest model

### Adding New Models

To add a new model, edit `CoMEM-Agent-Inference/agent/llm_config.py`:

```python
# In create_direct_vllm_model function
model_name_map = {
    'your-model': 'HuggingFace/model-name',
    ...
}

model_server_map = {
    'your-model': 'http://localhost:PORT/v1',
    ...
}
```

## üìÇ CoMEM-Agent-Inference Structure

```
GUI-Agent/
‚îú‚îÄ‚îÄ actions/              # Action creation and parsing
‚îú‚îÄ‚îÄ agent/                # Core agent implementation with ReAct
‚îú‚îÄ‚îÄ browser_env/          # Playwright-based browser environment
‚îú‚îÄ‚îÄ config/               # Configuration and argument parsing
‚îú‚îÄ‚îÄ data_preparation/     # Data preparation scripts
‚îú‚îÄ‚îÄ examples/             # Example scripts for running experiments
‚îú‚îÄ‚îÄ memory/               # Experience memory system (FAISS indexing)
‚îú‚îÄ‚îÄ memory_evolution/     # Data flywheel for memory expansion
‚îú‚îÄ‚îÄ Mind2Web_evaluation/  # Mind2Web benchmark evaluation
‚îú‚îÄ‚îÄ MMInA_evaluation/     # MMInA benchmark evaluation
‚îú‚îÄ‚îÄ mmina/                # MMInA dataset
‚îú‚îÄ‚îÄ tools/                # Function calling tools (GUI, search, analysis)
‚îú‚îÄ‚îÄ utils/                # Shared utilities and helpers
‚îú‚îÄ‚îÄ webvoyager_evaluation/# WebVoyager benchmark evaluation
‚îú‚îÄ‚îÄ run_baseline.sh       # Main evaluation script
‚îî‚îÄ‚îÄ run.py                # Python entry point
```

Each directory contains a detailed `README.md` with component documentation.

## üéì Training Continuous Memory

For training your own continuous memory models, please refer to [this folder]((CoMEM-Agent-train)) and our training repository:

**[CoMEM Training Repository](https://github.com/WenyiWU0111/CoMEM)**

The repository includes:
- Training scripts for Q-Former memory encoder
- Data synthesis pipeline
- Memory retrieval and indexing
- Evaluation protocols

Key training details:
- **Parameters**: Only 1.2% of the model (LoRA on Q-Former)
- **Memory Size**: 8 continuous embeddings per trajectory
- **Base Model**: Frozen during training and inference

## üìù Citation

If you find this work useful, please cite our paper:

```bibtex
@article{wu2025comemagent,
  title={Auto-Scaling Continuous Memory For GUI Agent},
  author={Wenyi Wu, Kun Zhou, Ruoxin Yuan, Vivian Yu, Stephen Wang, Zhiting Hu, Biwei Huang},
  journal={arXiv preprint arXiv:2510.09038},
  year={2025}
}
```

## üìß Contact

For questions or collaboration opportunities, please reach out through Email: wew058@ucsd.edu

---
